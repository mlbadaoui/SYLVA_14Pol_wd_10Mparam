{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------- Load packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Interact with the file system\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "import time # Time access and conversions\n",
    "import json # Transmit structured data interchange format\n",
    "import numpy as np # Work with arrays \n",
    "import pandas as pd # Data manipulation : pd.Dataframe\n",
    "import tensorflow as tf # To create deep learning models\n",
    "import tensorflow_datasets as tfds  \n",
    "from matplotlib import pyplot as plt # Added for conf_matrix\n",
    "import seaborn as sns # Added for conf_matrix\n",
    "from ImageHelper import blobFromImage, imageFromBlob # Pre-processing images for pre-trained keras model\n",
    "from backgroundGenerator import BackgroundGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------- configure access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0],'GPU')\n",
    "\n",
    "try: \n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=25000)]) \n",
    "except: \n",
    "    print(\"Invalid device or cannot modify virtual devices once initialized.\", flush=True) \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysqlSettings = {\n",
    "    \"db_url\": os.getenv('DB_URL', '10.182.129.115'),\n",
    "    \"db_port\": os.getenv('DB_PORT', 3309),\n",
    "    \"db_user\": os.getenv('DB_USER', 'root'),\n",
    "    \"db_pw\": os.getenv('DB_PW', '31415swisens')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global parameters (Make sure to choose a unique name for each run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName =\"14Pol_Rain_noSpores_second_10Mparam_test\"\n",
    "tensorboardLogFolder = \"/scratch/nina/logs\" \n",
    "checkpointFolder = \"/scratch/nina/checkpoints/\" + modelName + \"/\"\n",
    "confMatFolder = \"/scratch/nina/confusion_matrix/\" + modelName + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "chunksize = 256 # How many events should be used per dataset. TF will tain on them for x epochs before going to the next chunk of data. Choose size according to your hardware (ram, gpu, gpu-memory)\n",
    "chunkPrefetch = 2 # How many chunks should be cached in the background.\n",
    "batchsize = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "epochsPerDatasetChunk = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set this to True if you choose to also include FL to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_fluorescence = False\n",
    "n_fl_configs=26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    target_names = ['alnus',\n",
    "                    'betula',\n",
    "                    'carpinus',\n",
    "                    'corylus',\n",
    "                    'cupressus',\n",
    "                    'fagus',\n",
    "                    'fraxinus',\n",
    "                    'pinaceae',\n",
    "                    'platanus',\n",
    "                    'poaceae',\n",
    "                    'populus',\n",
    "                    'quercus',\n",
    "                    'taxus',                    \n",
    "                    'ulmus',\n",
    "                    #'alternaria',\n",
    "                    #'fusarium',\n",
    "                    'rain'                \n",
    "                   ]    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetList = [\n",
    "    #POLLEN DATASETS\n",
    "    '11ea8493-7107-8db4-9bf7-ae7b87f820b4',#0alnus 'alnus_20200220_p5_1_benoit' 3990\n",
    "    '11ea847a-f995-790c-830f-ae7b87f820b4',#0alnus 'alnus_20200218_p2_1_benoit' 4966\n",
    "    '11ea8475-957e-347c-985a-ae7b87f820b4',#0alnus 'alnus_20200214_p4_1_benoit' 3474 TOTAL ALNUS=12'430\n",
    "    '11ea8897-f50e-66a2-9876-ae7b87f820b4',#1betula 'betula_20200406_p2_1_benoit' 5770\n",
    "    '11ea8632-18ed-7210-985a-ae7b87f820b4',#1betula 'betula_20200407_p4_2_benoit' 6533\n",
    "    '11ea8632-1eb2-2452-bc84-ae7b87f820b4',#1betula 'betula_20200406_p4_1_benoit' 2173 TOTAL BETULA=14'476\n",
    "    '11ea8f77-4ee3-aef4-b330-ae7b87f820b4',#2carpinus 'carpinus_20200319_p5_2_fiona' 643\n",
    "    '11ea8f6d-3e75-9fe6-b46e-ae7b87f820b4',#2carpinus 'carpinus_20200319_p2_2_fiona' 664\n",
    "    '11ea8f6d-1562-211a-8192-ae7b87f820b4',#2carpinus 'carpinus_20200319_p2_3_fiona' 545\n",
    "    '11ea8f6c-b78c-d076-a542-ae7b87f820b4',#2carpinus 'carpinus_20200319_p4_2_fiona' 395 TOTAL CARPINUS=2'247\n",
    "    '11ea8498-b729-d4e6-bc84-ae7b87f820b4',#3corylus 'corylus_20200225_p2_2_benoit' 3736\n",
    "    '11ea8498-b083-cb92-a1a5-ae7b87f820b4',#3corylus 'corylus_20200225_p2_1_benoit' 500\n",
    "    '11ea8498-afa9-cec4-a877-ae7b87f820b4',#3corylus 'corylus_20200225_p5_1_benoit' 3578 TOTAL CORYLUS=7'814\n",
    "    '11ea8fa9-6c12-723a-b3dd-ae7b87f820b4',#4cupressus 'cupressus_20200317_p5_1_fiona' 421\n",
    "    '11ea8fa8-fafa-aeb4-ac46-ae7b87f820b4',#4cupressus 'cupressus_20200317_p2_1_fiona' 2340\n",
    "    '11ea8fa8-d163-dce2-b1cb-ae7b87f820b4',#4cupressus 'cupressus_20200317_p4_1_fiona' 583 TOTAL CUPRESSUS=3'344\n",
    "    '11ea8636-313b-a6e4-a69e-ae7b87f820b4',#5fagus 'fagus_20200413_p4_1_benoit' 2759\n",
    "    '11ea8635-ef91-6ab2-a877-ae7b87f820b4',#5fagus 'fagus_20200407_p2_1_benoit' 3410\n",
    "    '11ea8635-eb18-6ee0-9876-ae7b87f820b4',#5fagus 'fagus_20200413_p5_1_benoit' 4143 TOTAL FAGUS=10'312\n",
    "    '11ea857e-7bc5-60a0-842e-ae7b87f820b4',#6fraxinus 'fraxinus_20200402_p5_2_benoit' 5703\n",
    "    '11ea857b-3d52-9034-830f-ae7b87f820b4',#6fraxinus 'fraxinus_20200330_p4_1_benoit' 2621\n",
    "    '11ea857b-150e-c372-bc84-ae7b87f820b4',#6fraxinus 'fraxinus_20200330_p2_1_benoit' 1712 TOTAL FRAXINUS=10'036\n",
    "    '11ea8af3-c533-f39e-8b25-ae7b87f820b4',#7pinaceae 'picea_20200423_p2_1_fiona' 1826\n",
    "    '11ea8af1-91fc-9a46-8b25-ae7b87f820b4',#7pinaceae 'picea_20200423_p4_1_fiona' 2375\n",
    "    '11ea8af0-83dc-6d66-b06c-ae7b87f820b4',#7pinaceae 'picea_20200423_p5_1_fiona' 1969\n",
    "    '11ea863d-acf6-0ade-985a-ae7b87f820b4',#7pinaceae 'pinus_20200421_p5_1_benoit' 3403\n",
    "    '11ea863c-2449-be52-8814-ae7b87f820b4',#7pinaceae 'pinus_20200421_p2_1_benoit' 8582 TOTAL PINACEAE=18'155\n",
    "    '11ea8b83-25c9-8194-90d1-ae7b87f820b4',#8platanus 'platanus_20200417_p4_1_benoit' 5603\n",
    "    '11ea8881-3721-9aa8-a907-ae7b87f820b4',#8platanus 'platanus_20200417_p2_1_benoit' 5544 TOTAL PLATANUS=11'147\n",
    "    '11ea990f-ee01-8334-b3dd-ae7b87f820b4',#9poaceae 'gram_20200518_p2_1_benoit' 1229\n",
    "    '11ea990c-b2bc-fe96-b46e-ae7b87f820b4',#9poaceae 'gram_20200518_p5_1_benoit' 1508  TOTAL POACEAE inital=4'909 \n",
    "    '11eb5fd9-961a-313e-ac56-ae7b87f820b4',#9poaceae 'POCclean_cynosurus_20200520_p4_1_fiona' 5895\n",
    "    '11eb5fd9-dd36-0a20-88f3-ae7b87f820b4',#9poaceae 'POCclean_cynosurus_20200520_p2_1_' 6248 \n",
    "    '11ebe542-660e-0206-80be-ae7b87f820b4',#9poaceae 'poaceae_dactylis_fresh_p19_2021_tri_Nina' 3110\n",
    "    '11eb5fc3-03fa-6da2-8b42-ae7b87f820b4',#9poaceae 'POCclean_dactylis_20200518_p4_1' 1127\n",
    "    '11ebe540-187e-9a0c-b0e2-ae7b87f820b4',#9poaceae 'poaceae_trisetum_fresh_p19_2021_tri_Nina' 1377 \n",
    "    '11ea8893-edfb-ca84-a877-ae7b87f820b4',#10populus 'populus_20200327_p5_1_benoit' 657\n",
    "    '11ea84a0-e89b-43b8-a69e-ae7b87f820b4',#10populus 'populus_20200327_p2_benoit' 508\n",
    "    '11ea84a0-a2f0-ab8c-a877-ae7b87f820b4',#10populus 'populus_20200327_p4_benoit' 2913 TOTAL POPULUS=4'078\n",
    "    '11ea863e-1fea-0f7c-a1a5-ae7b87f820b4',#11quercus 'quercus_20200421_p4_1_benoit' 3824\n",
    "    '11ea863e-1b86-8226-a1a5-ae7b87f820b4',#11quercus 'quercus_20200421_p2_1_benoit' 4768\n",
    "    '11ea863d-f388-a038-a1a5-ae7b87f820b4',#11quercus 'quercus_20200421_p5_1_benoit' 2519 TOTAL QUERCUS=11'111\n",
    "    '11ea8477-cede-e7dc-897d-ae7b87f820b4',#12taxus 'taxus_20200218_p4_1_benoit' 4872\n",
    "    '11ea8477-b584-b690-830f-ae7b87f820b4',#12taxus 'taxus_20200218_p2_1_benoit' 5593\n",
    "    '11ea8494-33a5-2e4e-bc84-ae7b87f820b4',#12taxus 'taxus_20200220_p5_1_benoit' 3411 TOTAL TAXUS=13'876    \n",
    "    '11ea849c-df8f-d95e-897d-ae7b87f820b4',#13ulmus 'ulmus_20200311_p4_2_benoit' 3289\n",
    "    '11ea849c-db7b-2170-8b0f-ae7b87f820b4',#13ulmus 'ulmus_20200311_p2_2_benoit' 2392\n",
    "    '11ea849a-0e25-4018-8814-ae7b87f820b4',#13ulmus 'ulmus_20200304_p5_1_benoit' 4844 TOTAL ULMUS=10'525    \n",
    "    # SPORES DATASETS\n",
    "    #'11ebf9db-f2e9-98cc-bc67-ae7b87f820b4',#14Alternaria solani 'alternaria_solani_sophie_clean' event counts 2767  \n",
    "    #'11ec01b9-d571-ea8e-b7e1-ae7b87f820b4',#15Fusarium graminearum 'fusarium_graminearum_p1' event count 25054\n",
    "    # RAIN DATASETS\n",
    "    '11ebe542-f782-c172-bf10-ae7b87f820b4',#16Rain 'P5_Payerne_Rain_28_04' event counts 389\n",
    "    '11ebeabd-e224-d5c4-8b63-ae7b87f820b4',#16Rain 'P5_Payerne_Rain_30_04_AM' event counts 2786\n",
    "    '11ebedec-0da5-47ac-8066-ae7b87f820b4',#16Rain 'P5_Payerne_Rain_30_04_PM' event counts 3179\n",
    "    '11ebee15-1fea-4c68-9cd6-ae7b87f820b4'#16Rain 'P16_Locarno_Rain_29_04' event counts 7691 TOTAL PLUIE = 14045\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList = [ # Labels corresponding to the datasetList, NB: labels pluie added\n",
    "    0,\n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    1,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    2,\n",
    "    3,\n",
    "    3,\n",
    "    3,\n",
    "    4,\n",
    "    4,\n",
    "    4,\n",
    "    5,\n",
    "    5,\n",
    "    5,\n",
    "    6,\n",
    "    6,\n",
    "    6,\n",
    "    7,\n",
    "    7,\n",
    "    7,\n",
    "    7,\n",
    "    7,\n",
    "    8,\n",
    "    8,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    9,\n",
    "    10,\n",
    "    10,\n",
    "    10,\n",
    "    11,\n",
    "    11,\n",
    "    11,\n",
    "    12,\n",
    "    12,\n",
    "    12,\n",
    "    13,\n",
    "    13,\n",
    "    13,\n",
    "    #14,\n",
    "    #15,\n",
    "    14,\n",
    "    14,\n",
    "    14,\n",
    "    14\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure testset:<br>\n",
    "Valid values:<br>\n",
    "fromFirstChunk:   First chunk of data is used as test set<br>\n",
    "fromDataset:      Get test data from a dataset defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetMode = \"fromFirstChunk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the datasets for testsetMode=\"fromDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsetList = [\n",
    "    ]\n",
    "testsetLabels = [ # Labels corresponding to the testsetList\n",
    "    0,\n",
    "    1,\n",
    "    2,\n",
    "    3,\n",
    "    4,\n",
    "    5,\n",
    "    6,\n",
    "    7,\n",
    "    8,\n",
    "    9,\n",
    "    10,\n",
    "    11,\n",
    "    12,\n",
    "    13, \n",
    "    #14,\n",
    "    #15, \n",
    "    14 \n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFlInput(input):\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(input)\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(path)\n",
    "    path = tf.keras.layers.MaxPool1D(2)(path)\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(path)\n",
    "    path = tf.keras.layers.Conv1D(32, 3)(path)\n",
    "    path = tf.keras.layers.MaxPool1D(2)(path)\n",
    "    path = tf.keras.layers.Flatten()(path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model(nClasses, with_fluorescence=False, n_fl_configs=1,strategy=strategy):\n",
    "    with strategy.scope():\n",
    "\n",
    "        in_img0 = tf.keras.layers.Input((200,200,1))\n",
    "        in_img1 = tf.keras.layers.Input((200,200,1))\n",
    "    \n",
    "        # If you want to train a model including fluorescence, you need to include these inputs in your model\n",
    "        if with_fluorescence:\n",
    "            in_fl_avg = tf.keras.layers.Input((n_fl_configs*6, 1))\n",
    "            in_fl_pha = tf.keras.layers.Input((n_fl_configs*6, 1))\n",
    "            in_fl_corrMag = tf.keras.layers.Input((n_fl_configs*6, 1))\n",
    "\n",
    "        # Define your model here!\n",
    "\n",
    "        #Image Processing\n",
    "        path1 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(in_img0)\n",
    "        path1 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.2)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.2)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.2)(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path1)\n",
    "        path1 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path1)\n",
    "        path1 = tf.keras.layers.Dropout(0.2)(path1)\n",
    "\n",
    "        #path1 = tf.keras.layers.Dropout(0.3)(path1)\n",
    "        #path1 = tf.keras.layers.Dropout(0.4)(path1)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(in_img1)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (5,5), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.2)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D(2, strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.2)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(128, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.2)(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.Conv2D(256, (3,3), padding='same', activation='relu')(path2)\n",
    "        path2 = tf.keras.layers.MaxPool2D((2,2), strides=(2,2),padding='same')(path2)\n",
    "        path2 = tf.keras.layers.Dropout(0.2)(path2)\n",
    "        #path2 = tf.keras.layers.Dropout(0.3)(path2)\n",
    "\n",
    "        path1Flat = tf.keras.layers.Flatten()(path1)\n",
    "        path2Flat = tf.keras.layers.Flatten()(path2)\n",
    "\n",
    "        # FL Processing\n",
    "        if with_fluorescence:\n",
    "            fl_avg_path = processFlInput(in_fl_avg)\n",
    "            fl_pha_path = processFlInput(in_fl_pha)\n",
    "            fl_corrMag_path = processFlInput(in_fl_corrMag)\n",
    "            path = tf.keras.layers.Concatenate()(\n",
    "                [path1Flat, path2Flat, fl_avg_path, fl_pha_path, fl_corrMag_path]\n",
    "            )\n",
    "        else:\n",
    "            path = tf.keras.layers.Concatenate()([path1Flat, path2Flat])\n",
    "\n",
    "        #Densely(fully)-connected layer\n",
    "        path = tf.keras.layers.Dense(64)(path)\n",
    "        path = tf.keras.layers.Dropout(0.2)(path)\n",
    "        #Densely(fully)-connected layer\n",
    "        path = tf.keras.layers.Dense(nClasses)(path)\n",
    "        #Softmax activation fct\n",
    "        output = tf.keras.layers.Softmax()(path)\n",
    "    \n",
    "        # If we work with fluorescence, we need to add all the inputs to the final model\n",
    "        if with_fluorescence:\n",
    "            model = tf.keras.Model(\n",
    "                inputs=[in_img0, in_img1, in_fl_avg, in_fl_pha, in_fl_corrMag],\n",
    "                outputs=output\n",
    "            )\n",
    "        else:\n",
    "            model = tf.keras.Model(inputs=[in_img0, in_img1], outputs=output)\n",
    "        \n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.00005)\n",
    "        model.compile(optimizer=opt,\n",
    "                        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                        metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################## DO NOT CHANGE CODE AFTER THIS LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrepareFunc(with_fluorescence, label):\n",
    "    def processFLColumn(x, mapping=lambda x: x):\n",
    "        x = json.loads(x)\n",
    "        result = []\n",
    "        i = 0\n",
    "        while str(i) in x:\n",
    "            result.extend(x[str(i)])\n",
    "            i += 1\n",
    "        result = [mapping(a) for a in result]\n",
    "        return result\n",
    "    def processDf(df):\n",
    "        df[\"img0\"] = df[\"img0\"].apply(imageFromBlob)\n",
    "        df[\"img0\"] = df[\"img0\"].apply(lambda x: np.array(x, dtype=np.float))\n",
    "        df[\"img0\"] = df[\"img0\"].apply(lambda x: x/(2**16-1))\n",
    "        \n",
    "        df[\"img1\"] = df[\"img1\"].apply(imageFromBlob)\n",
    "        df[\"img1\"] = df[\"img1\"].apply(lambda x: np.array(x, dtype=np.float))\n",
    "        df[\"img1\"] = df[\"img1\"].apply(lambda x: x/(2**16-1))\n",
    "        if with_fluorescence:\n",
    "            df[\"avg\"] = df[\"avg\"].apply(\n",
    "                processFLColumn,\n",
    "                mapping = lambda x: x/0.5\n",
    "            )\n",
    "            df[\"corrPha\"] = df[\"corrPha\"].apply(\n",
    "                processFLColumn,\n",
    "                mapping = lambda x: x/np.pi\n",
    "            )\n",
    "            df[\"corrMag\"] = df[\"corrMag\"].apply(\n",
    "                processFLColumn,\n",
    "                mapping = lambda x: x/0.5\n",
    "            )\n",
    "        df[\"label\"] = label\n",
    "        return df\n",
    "    return processDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetFromItList(itList, num_classes, batchsize, first=False):\n",
    "    df = None\n",
    "    for i, it in enumerate(itList):\n",
    "        if first:\n",
    "            dfTmp : pd.DataFrame = it.getFirst()\n",
    "        else:\n",
    "            dfTmp : pd.DataFrame = next(it)\n",
    "        if df is None:\n",
    "            df = dfTmp\n",
    "        else:\n",
    "            df = df.append(dfTmp)\n",
    "    print(\"Randomizing the sample in the set\", flush=True)\n",
    "    df = df.sample(frac=1).reset_index(drop=True) # NB df contains 12000 events(=250[events/(chunk*dataset)]*48[datasets])\n",
    "    print(\"Building TF-Dataset\", flush=True)\n",
    "    if with_fluorescence:\n",
    "        datasetData = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                np.array(df[\"img0\"].to_list()).reshape((len(df),200,200,1)), \n",
    "                np.array(df[\"img1\"].to_list()).reshape((len(df),200,200,1)),\n",
    "                np.array(df[\"avg\"].to_list()).reshape((len(df), n_fl_configs*6, 1)),\n",
    "                np.array(df[\"corrPha\"].to_list()).reshape((len(df), n_fl_configs*6, 1)),\n",
    "                np.array(df[\"corrMag\"].to_list()).reshape((len(df), n_fl_configs*6, 1))\n",
    "            ))\n",
    "    else:\n",
    "        datasetData = tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                np.array(df[\"img0\"].to_list()).reshape((len(df),200,200,1)), \n",
    "                np.array(df[\"img1\"].to_list()).reshape((len(df),200,200,1))\n",
    "            ))\n",
    "    datasetLabels = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.one_hot(df[\"label\"].values, num_classes)\n",
    "        ))\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((datasetData, datasetLabels)).batch(batchsize) #NB dataset into batch\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  139\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  133\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  252\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  165\n",
      "next batch elements:  33\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "Iterator Empty!!\n",
      "Restarting iteratornext batch elements: \n",
      " 131\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  244\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  152\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  \n",
      "256next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  133\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  71\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  139\n",
      "next batch elements:  256\n",
      "next batch elements:  256next batch elements: \n",
      " 256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256next batch elements: \n",
      " 256\n",
      "next batch elements:  256\n",
      "next batch elements: next batch elements:  256 \n",
      "256\n",
      "next batch elements:  256\n",
      "next batch elements:  145\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  252\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  165\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  244\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256next batch elements: \n",
      " 256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  next batch elements: 256 \n",
      "256\n",
      "next batch elements:  next batch elements: 133 \n",
      "256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements: next batch elements:   256256\n",
      "\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  139\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  252\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  244\n",
      "next batch elements:  165\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  131\n",
      "next batch elements: next batch elements:   256152\n",
      "\n",
      "next batch elements:  33\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  139\n",
      "next batch elements:  256\n",
      "next batch elements:  71\n",
      "next batch elements:  244\n",
      "next batch elements:  256\n",
      "next batch elements:  165\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  205\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  103\n",
      "next batch elements:  next batch elements: 145\n",
      " 256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  252\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "Iterator Empty!!\n",
      "Restarting iterator\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  256\n",
      "next batch elements:  133\n",
      "next batch elements:  256\n"
     ]
    }
   ],
   "source": [
    "itList = []\n",
    "for i, dataset in enumerate(datasetList):\n",
    "    itList.append(\n",
    "        BackgroundGenerator(\n",
    "            dataset,\n",
    "            with_fl=with_fluorescence,\n",
    "            prefetch=chunkPrefetch,\n",
    "            mysqlSettings=mysqlSettings, \n",
    "            chunksize=chunksize,\n",
    "            reserveFirst= testsetMode==\"fromFirstChunk\",\n",
    "            prepareFunc=getPrepareFunc(with_fluorescence=with_fluorescence, label=labelList[i])\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testsetMode==\"fromDataset\":\n",
    "    testItList = []\n",
    "    for i, dataset in enumerate(testsetList):\n",
    "        testItList.append(\n",
    "            BackgroundGenerator(dataset, mysqlSettings=mysqlSettings, chunksize=chunksize, prepareFunc=getPrepareFunc(with_fluorescence=with_fluorescence, label=labelList[i]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterators are built\n"
     ]
    }
   ],
   "source": [
    "print(\"Iterators are built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model is built:\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 200, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 200, 200, 64) 1664        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 200, 200, 64) 1664        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 200, 200, 64) 102464      conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 200, 200, 64) 102464      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 100, 100, 64) 0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 100, 100, 64) 0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 100, 100, 64) 0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100, 100, 64) 0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 100, 100, 64) 36928       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 100, 100, 64) 36928       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 100, 100, 64) 36928       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 100, 100, 64) 36928       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 50, 50, 64)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 50, 50, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 50, 64)   0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 50, 50, 64)   0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 50, 50, 128)  73856       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 50, 50, 128)  73856       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 50, 50, 128)  147584      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 50, 50, 128)  147584      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 50, 50, 128)  147584      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 25, 25, 128)  0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 25, 25, 128)  0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 25, 25, 128)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 25, 25, 128)  0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 25, 25, 256)  295168      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 25, 25, 256)  295168      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 25, 25, 256)  590080      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 25, 25, 256)  590080      conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 25, 25, 256)  590080      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 25, 25, 256)  590080      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 13, 13, 256)  0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 13, 13, 256)  0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 13, 13, 256)  0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 13, 13, 256)  0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 43264)        0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 43264)        0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 86528)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           5537856     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 15)           975         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (None, 15)           0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,583,503\n",
      "Trainable params: 9,583,503\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model...\", flush=True)\n",
    "model = get_compiled_model(NUM_CLASSES, with_fluorescence=with_fluorescence, n_fl_configs=n_fl_configs,strategy = strategy)\n",
    "print(\"Model is built:\", flush=True)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = tf.keras.callbacks.TensorBoard(log_dir=f\"{tensorboardLogFolder}/{modelName}\")\n",
    "saver = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointFolder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building testset\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Fetching first data...\n",
      "Data ready\n",
      "Randomizing the sample in the set\n",
      "Building TF-Dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Building testset\", flush=True)\n",
    "if testsetMode==\"fromFirstChunk\":\n",
    "    testset = datasetFromItList(itList=itList, batchsize=batchsize, num_classes=NUM_CLASSES, first=True)\n",
    "if testsetMode==\"fromDataset\":\n",
    "    testset = datasetFromItList(itList=testItList, batchsize=batchsize, num_classes=NUM_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oss, acc = model.e valuate(testset, verbose=2)<br>\n",
    "odel.load_weights(f\"{checkpointFolder}/{modelName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare next chunk for training...\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "fetching data\n",
      "done\n",
      "Randomizing the sample in the set\n",
      "Building TF-Dataset\n",
      "Training model on the current TF-Dataset (nr: 0)\n",
      "Epoch 1/5\n",
      " 26/203 [==>...........................] - ETA: 25s - loss: 2.6730 - accuracy: 0.1478"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1196576/3417423751.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrainingSet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetFromItList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitList\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training model on the current TF-Dataset (nr: {datasetCounter})\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochsPerDatasetChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# CONFUSION MATRIX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \"\"\"\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.9.5/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasetCounter = 0\n",
    "bestAccuracy = 0.00\n",
    "bestValAccuracy = 0.00\n",
    "confusion_final = np.zeros((NUM_CLASSES,NUM_CLASSES)) # NB: added bench.py (sophie)\n",
    "while True:\n",
    "    \n",
    "    # Reset confusion matrix to zeros every 5 loops\n",
    "    if (datasetCounter % 5 == 0):\n",
    "        confusion_final = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "        \n",
    "    confusion_temp = np.zeros((NUM_CLASSES,NUM_CLASSES))\n",
    "    print(\"Prepare next chunk for training...\", flush=True)\n",
    "    trainingSet = datasetFromItList(itList=itList, batchsize=batchsize, num_classes=NUM_CLASSES)\n",
    "    print(f\"Training model on the current TF-Dataset (nr: {datasetCounter})\", flush=True)\n",
    "    history=model.fit(trainingSet, validation_data=testset,  epochs=epochsPerDatasetChunk, verbose=1, callbacks=[logger, saver]) \n",
    "\n",
    "    # CONFUSION MATRIX\n",
    "    Y_pred = model.predict(testset)\n",
    "    y_pred = np.argmax(Y_pred, axis = 1)\n",
    "    true_categories = tf.concat([y for x, y in testset], axis=0)\n",
    "    np_testset = tfds.as_numpy(true_categories)\n",
    "    np_testset = np.argmax(np_testset,axis=1)\n",
    "    print('True_class argmax')\n",
    "    print(np_testset[0:chunksize])\n",
    "    print('Pred_class_argmax')\n",
    "    print(y_pred[0:chunksize])   \n",
    "    print('Confusion Matrix')\n",
    "    confusion_temp=confusion_matrix(y_pred = y_pred, y_true = np_testset)\n",
    "    confusion_final=(confusion_final+confusion_temp)\n",
    "    df_conf_mat = pd.DataFrame(confusion_final, columns = target_names, index = target_names)\n",
    "    # normalized confusion matrix\n",
    "    confusion_final_norm = np.around(confusion_final.astype('float') / confusion_final.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    df_conf_mat_norm = pd.DataFrame(confusion_final_norm, columns = target_names, index = target_names)\n",
    "    print(df_conf_mat_norm)\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(df_conf_mat_norm, annot=True,cmap=plt.cm.Blues)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "          \n",
    "    # Retrieve accuracy of this while loop for storing best weights\n",
    "    accuraciesOfThisLoop = history.history['accuracy']\n",
    "    valAccuraciesOfThisLoop = history.history['val_accuracy']\n",
    "    # Compare the accuarcy and valAcc of the last epoch with bestAccuracy      \n",
    "    idx = len(accuraciesOfThisLoop)-1\n",
    "    print(\"Loop while number: \", datasetCounter, \"current bestAccuracy: \", bestAccuracy, \"accuraciesOfThisLoop : \", accuraciesOfThisLoop[idx], \"valAccuraciesOfThisLoop: \", valAccuraciesOfThisLoop[idx], \"acc - val_acc: \", (accuraciesOfThisLoop[idx] - valAccuraciesOfThisLoop[idx]))\n",
    "    if (bestAccuracy < accuraciesOfThisLoop[idx]) and ((accuraciesOfThisLoop[idx] - valAccuraciesOfThisLoop[idx]) < 0.1):\n",
    "        \n",
    "        bestAccuracy = '{:.3f}'.format(round(valAccuraciesOfThisLoop[idx], 3))\n",
    "        bestAccuracy = float(bestAccuracy)\n",
    "        bestValAccuracy = '{:.3f}'.format(round(valAccuraciesOfThisLoop[idx], 3))\n",
    "        bestValAccuracy = float(bestValAccuracy)          \n",
    "        print(\"The weights are saved...\")        \n",
    "        model.save_weights(checkpointFolder + \"weights.best.hdf5\")\n",
    "        print(\"The confusion matrix is saved...\")       \n",
    "        figure.savefig(confMatFolder +'Conf_mat_valAcc_'+str(bestValAccuracy)+'.jpg', bbox_inches = 'tight')\n",
    "        print('\\n')\n",
    "    \n",
    "    datasetCounter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
